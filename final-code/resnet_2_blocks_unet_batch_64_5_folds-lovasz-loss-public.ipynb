{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "00833d394e3069216af171fd979c814e7e1e430d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from skimage.transform import resize,rotate\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras import backend as K\n",
    "from keras.losses import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "0e26e21ff39e8b2afc0003fec4e4f5269f61aa4c"
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "n_pixels = 101\n",
    "channels = 1\n",
    "path_train = 'train/'\n",
    "path_test = 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "034cbfecacf915fdef0f19572cbf3401ee8879c8"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c672d9bf3127dd10ed54e2e358887913f7ba6684"
   },
   "outputs": [],
   "source": [
    "sample=pd.read_csv(\"sample_submission.csv\")\n",
    "print(sample.shape)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62af04fae70adc7d449fd6bc8ae7a7668941562d"
   },
   "outputs": [],
   "source": [
    "depth=pd.read_csv(\"depths.csv\")\n",
    "print(depth.shape)\n",
    "depth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11da35d0fd003b90f3f37f5a07a52f0fe2fd00c9"
   },
   "outputs": [],
   "source": [
    "train_ids = train.id.values\n",
    "test_ids = sample.id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8f02165966489c8a21bb7127bb88e7cf607599d"
   },
   "outputs": [],
   "source": [
    "resize_mode='constant'\n",
    "\n",
    "# Get and resize train images and masks\n",
    "X = np.zeros((len(train_ids), n_pixels, n_pixels, channels), dtype=np.uint8)\n",
    "y = np.zeros((len(train_ids), n_pixels, n_pixels, 1), dtype=np.bool)\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "for n, id_ in tqdm.tqdm_notebook(enumerate(train_ids),total=len(train_ids)):\n",
    "    path = path_train\n",
    "    x = np.array(load_img(path + '/images/' + id_+\".png\",color_mode='grayscale'))\n",
    "    X[n] = x.reshape(n_pixels,n_pixels,channels)\n",
    "    mask = np.array(load_img(path + '/masks/' + id_+\".png\",color_mode='grayscale'))\n",
    "    y[n] = mask.reshape(n_pixels,n_pixels,1)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cefe7006f43d5d060eb1a7cfbb365191884560f1"
   },
   "source": [
    "## TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "179750f71645a40ec0e09fdfb4376a8614c6a612",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get and resize test images\n",
    "X_test = np.zeros((len(test_ids),n_pixels,n_pixels,channels), dtype=np.uint8)\n",
    "sizes_test = []\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm.tqdm_notebook(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = path_test\n",
    "    x= np.array(load_img(path + '/images/' + id_+\".png\",color_mode='grayscale'))\n",
    "    X_test[n] = x.reshape(n_pixels,n_pixels,1)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fd71cdff9d3c0ef00d54e7e58e40a7808bdf8be"
   },
   "outputs": [],
   "source": [
    "# storing empty images \n",
    "\n",
    "\n",
    "p=np.ones((4000,))\n",
    "for i in range(4000):\n",
    "    p[i]=np.sum(X[i,:,:,:])/(128*128)\n",
    "\n",
    "trainindex=[]\n",
    "for j,i in enumerate(p):\n",
    "    if i==0:\n",
    "        trainindex.append(j)\n",
    "        \n",
    "        \n",
    "print(len(trainindex))\n",
    "print(trainindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing empty images\n",
    "\n",
    "q=np.ones((18000,))\n",
    "for i in range(18000):\n",
    "    q[i]=np.sum(X_test[i,:,:,:])/(128*128)\n",
    "    \n",
    "indextest=[]\n",
    "for j,i in enumerate(q):\n",
    "    if i==0:\n",
    "        indextest.append(j)\n",
    "        \n",
    "print(len(indextest))\n",
    "print(indextest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ecd00b0e4251e0a88761ca157f95498341e5f5f",
    "collapsed": true
   },
   "source": [
    "## STRATIFICATION\n",
    "\n",
    "Calculating the salt coverage class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bda2b104818bd2a34e38a8a4656becf347b54b76"
   },
   "outputs": [],
   "source": [
    "coverage=np.zeros((train_ids.shape[0],))\n",
    "for i,name in tqdm.tqdm_notebook(enumerate(train_ids),total=train_ids.shape[0]):\n",
    "    coverage[i]=np.sum(y[i,:,:,0])\n",
    "\n",
    "train['coverage']=pd.Series(coverage)/y.shape[1]**2\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28b1d09674c3784267429f18f4f30b92f8f14682"
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train[\"coverage_class\"] = train.coverage.apply(cov_to_class)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "133f6b4e96c49766aa555faf9d9a9503a85827b5"
   },
   "outputs": [],
   "source": [
    "## coverage class numpy array\n",
    "coverage=train.coverage_class.values\n",
    "print(coverage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02b012df4e8a5d8a9c9e5d733ffc9a75093e98c"
   },
   "outputs": [],
   "source": [
    "train.loc[train['coverage']==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4a5ac159e01b945415d78d4844d613cf3f6b992"
   },
   "source": [
    "## AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aeebc6071c9c8529b3eda705cd5d394a0cf868ad"
   },
   "outputs": [],
   "source": [
    "ind=3\n",
    "\n",
    "p=rotate(X[ind,:,:,0].astype(np.uint8),angle=10,order=1,mode='reflect',\n",
    "                clip=True,preserve_range=True)\n",
    "q=rotate(y[ind,:,:,0].astype(np.uint8),angle=10,order=1,mode='reflect',\n",
    "         clip=True,preserve_range=True)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(X[ind].reshape(101,101),cmap='gist_gray')\n",
    "plt.subplot(222)\n",
    "plt.imshow(y[ind].reshape(101,101),cmap='gist_gray')\n",
    "plt.subplot(223)\n",
    "plt.imshow(p.astype(np.uint8).reshape(101,101),cmap='gist_gray')\n",
    "plt.subplot(224)\n",
    "plt.imshow(q.astype(np.uint8).reshape(101,101),cmap='gist_gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a5009a15fbe888646de4c8b566c0a4f8b0a8c2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "913d38293e2656a6141818842880a39043429070"
   },
   "outputs": [],
   "source": [
    "def crop_rescale_image(image,mask,state,limit):\n",
    "    np.random.seed(state)\n",
    "    h,w=image.shape[0],image.shape[1]\n",
    "    dy=int(h*limit)\n",
    "    y0=np.random.randint(int(0.1*h),dy)\n",
    "    y1=h-y0\n",
    "    \n",
    "    \n",
    "    np.random.seed(state+5000)\n",
    "    dx=int(w*limit)\n",
    "    x0=np.random.randint(int(0.1*w),dx)\n",
    "    x1=w-x0\n",
    "    \n",
    "    m='reflect'\n",
    "#     print(y0,x0)\n",
    "    cropped_image=resize(image[y0:y1,x0:x1,:],(h,w),mode=m,\n",
    "                         preserve_range=True,anti_aliasing=False)\n",
    "    cropped_mask=resize(mask[y0:y1,x0:x1,:],(h,w),mode=m,\n",
    "                        preserve_range=True,anti_aliasing=False)\n",
    "\n",
    "    \n",
    "    return cropped_image,cropped_mask\n",
    "    \n",
    "def random_crop_rescale(X,y,limit):\n",
    "    m=X.shape[0]\n",
    "    processed_image=np.zeros(X.shape)\n",
    "    processed_mask=np.zeros(y.shape)\n",
    "    for i in range(m):\n",
    "        processed_image[i,:,:,:],processed_mask[i,:,:,:]=crop_rescale_image(X[i,:,:,:],y[i,:,:,:],i,limit)\n",
    "    \n",
    "    return processed_image.astype(np.uint8),processed_mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "def rotate_grp(images,masks,ang):\n",
    "    rotated_images=np.zeros((images.shape))\n",
    "    rotated_masks=np.zeros((masks.shape))\n",
    "    for i in range(images.shape[0]):\n",
    "        rotated_images[i,:,:,0]=rotate(X[i,:,:,0].astype(np.uint8),angle=ang,order=1,mode='reflect',\n",
    "                clip=True,preserve_range=True)\n",
    "        rotated_masks[i,:,:,0]=rotate(y[i,:,:,0].astype(np.uint8),angle=ang,order=1,mode='reflect',\n",
    "         clip=True,preserve_range=True)\n",
    "        \n",
    "        \n",
    "    return rotated_images.astype(np.uint8),rotated_masks.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ef5193b2cd45f02e569ecee5ae7bfc7c9b5354d"
   },
   "outputs": [],
   "source": [
    "def augment(X,y,l,start_crop,end_crop,start_rot,end_rot,start_roll=-1,end_roll=-1,rolling=False):\n",
    "    m=X.shape[0]\n",
    "    print(\"Flipping\")\n",
    "    X=np.append(X,[np.fliplr(x) for x in X],axis=0)\n",
    "    y=np.append(y,[np.fliplr(i) for i in y],axis=0)\n",
    "\n",
    "    # trying random crop and rescale\n",
    "    np.random.seed(42)\n",
    "    index=np.arange(0,m)\n",
    "    # shuffling the array\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    print(\"Random crop and resize\")\n",
    "    # random crop and resize\n",
    "    processed_X,processed_y=random_crop_rescale(X[index[start_crop:end_crop]],\n",
    "                                                y[index[start_crop:end_crop]],limit=l)\n",
    "    X=np.append(X,processed_X,axis=0)\n",
    "    y=np.append(y,processed_y,axis=0)\n",
    "\n",
    "    print(\"Rotations\")\n",
    "    # 10 degree rotation\n",
    "    rotated_X,rotated_y=rotate_grp(X[index[start_rot:end_rot]],\n",
    "                                   y[index[start_rot:end_rot]],ang=10)\n",
    "\n",
    "    X=np.append(X,rotated_X,axis=0)\n",
    "    y=np.append(y,rotated_y,axis=0)\n",
    "    \n",
    "    if rolling:\n",
    "        print(\"Rolling\")\n",
    "        \n",
    "        roll_index=np.arange(0,X.shape[0])\n",
    "        \n",
    "        # shuffling\n",
    "        np.random.shuffle(roll_index)\n",
    "        \n",
    "        # rolling train and mask\n",
    "        to_roll_X=X[roll_index[start_roll:end_roll]]\n",
    "        to_roll_y=y[roll_index[start_roll:end_roll]]\n",
    "            \n",
    "        # 40 steps rolling\n",
    "        X=np.append(X,[np.roll(x, 40, axis = 1) for x in to_roll_X], axis = 0)\n",
    "        y=np.append(y,[np.roll(t,40,axis=1) for t in to_roll_y],axis=0)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "783dc7661f9f3f6218ff30515c026a6bca33d589"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "251709e9dea2da547c12cc6ea3df309599b1843f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4716a2112dfb71c75e60bff90cb17836f78bf66"
   },
   "outputs": [],
   "source": [
    "# metric\n",
    "\n",
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch]>0, B[batch]>0\n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "        \n",
    "    return np.mean(metric)\n",
    "\n",
    "def competition_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "        \n",
    "# for lovasz the threshold is zero\n",
    "def metric_for_lovasz(label,pred):\n",
    "    return tf.py_func(get_iou_vector,[label,pred>0],tf.float64)\n",
    "\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c5c2ba23becc85155a45e34af9e97b9c226b9c0"
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "# here i will add dice and bce loss\n",
    "# taken from https://www.kaggle.com/kmader/u-net-with-dice-and-augmentation\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    score=K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "    return 1.0-score\n",
    "\n",
    "def dice_and_bce(in_gt, in_pred):\n",
    "    \"\"\"combine DICE and BCE\"\"\"\n",
    "    return binary_crossentropy(in_gt, in_pred) + dice_loss(in_gt, in_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOVASZ LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code download from: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7eaaa7846d0509faff243969f0372899d3399abc"
   },
   "source": [
    "## ARCHITECUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b49a0fd646b457bd628ceba37a79c5eea0e269d"
   },
   "outputs": [],
   "source": [
    "def BatchActivate(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    if activation==True: x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, batch_activate=False):\n",
    "    x = BatchActivate(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3))\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    if batch_activate: x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def model_call():\n",
    "    inputs = Input(shape = (n_pixels,n_pixels, channels))\n",
    "\n",
    "    normalize=True\n",
    "    batch_normalize=True\n",
    "\n",
    "    if normalize:\n",
    "        s=Lambda(lambda x: x / 255) (inputs)\n",
    "    else:\n",
    "        s=inputs\n",
    "    \n",
    "    DropoutRatio=0.5\n",
    "    start_neurons=16\n",
    "    # 101 -> 50\n",
    "    conv1 = Conv2D(start_neurons*1, (3,3), activation=None, padding='same')(inputs)\n",
    "    conv1 = residual_block(conv1, start_neurons*1)\n",
    "    conv1 = residual_block(conv1, start_neurons*1, True)\n",
    "    pool1 = MaxPooling2D((2,2))(conv1)\n",
    "    pool1 = Dropout(DropoutRatio/2)(pool1)\n",
    "    \n",
    "    # 50 -> 25\n",
    "    conv2 = Conv2D(start_neurons*2, (3,3), activation=None, padding='same')(pool1)\n",
    "    conv2 = residual_block(conv2, start_neurons*2)\n",
    "    conv2 = residual_block(conv2, start_neurons*2, True)\n",
    "    pool2 = MaxPooling2D((2,2))(conv2)\n",
    "    pool2 = Dropout(DropoutRatio)(pool2)\n",
    "    \n",
    "    # 25 -> 12\n",
    "    conv3 = Conv2D(start_neurons*4, (3,3), activation=None, padding='same')(pool2)\n",
    "    conv3 = residual_block(conv3, start_neurons*4)\n",
    "    conv3 = residual_block(conv3, start_neurons*4, True)\n",
    "    pool3 = MaxPooling2D((2,2))(conv3)\n",
    "    pool3 = Dropout(DropoutRatio)(pool3)\n",
    "    \n",
    "    # 12 -> 6\n",
    "    conv4 = Conv2D(start_neurons*8, (3,3), activation=None, padding='same')(pool3)\n",
    "    conv4 = residual_block(conv4, start_neurons*8)\n",
    "    conv4 = residual_block(conv4, start_neurons*8, True)\n",
    "    pool4 = MaxPooling2D((2,2))(conv4)\n",
    "    pool4 = Dropout(DropoutRatio)(pool4)\n",
    "    \n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons*16, (3,3), activation=None, padding='same')(pool4)\n",
    "    convm = residual_block(convm, start_neurons*16)\n",
    "    convm = residual_block(convm, start_neurons*16, True)\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4 = Conv2DTranspose(start_neurons*8, (3,3), strides=(2,2), padding='same')(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons*8, (3,3), activation=None, padding='same')(uconv4)\n",
    "    uconv4 = residual_block(uconv4, start_neurons*8)\n",
    "    uconv4 = residual_block(uconv4, start_neurons*8, True)\n",
    "    \n",
    "    # 12 -> 25\n",
    "    deconv3 = Conv2DTranspose(start_neurons*4, (3,3), strides=(2,2), padding='valid')(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons*4, (3,3), activation=None, padding='same')(uconv3)\n",
    "    uconv3 = residual_block(uconv3, start_neurons*4)\n",
    "    uconv3 = residual_block(uconv3, start_neurons*4, True)\n",
    "    \n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons*2, (3,3), strides=(2,2), padding='same')(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    \n",
    "    uconv2 = Conv2D(start_neurons*2, (3,3), activation=None, padding='same')(uconv2)\n",
    "    uconv2 = residual_block(uconv2, start_neurons*2)\n",
    "    uconv2 = residual_block(uconv2, start_neurons*2, True)\n",
    "    \n",
    "    # 50 -> 101\n",
    "    deconv1 = Conv2DTranspose(start_neurons*1, (3,3), strides=(2,2), padding='valid')(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    \n",
    "    uconv1 = Conv2D(start_neurons*1, (3,3), activation=None, padding='same')(uconv1)\n",
    "    uconv1 = residual_block(uconv1, start_neurons*1)\n",
    "    uconv1 = residual_block(uconv1, start_neurons*1, True)\n",
    "    \n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding='same', activation=None)(uconv1)\n",
    "    outputs = Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    model = Model(inputs = [inputs], outputs = [outputs])\n",
    "    optimizer = Adam()\n",
    "\n",
    "    model.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [competition_metric])\n",
    "#     model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZATIONS AND FIT DATA FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58e87797db5bb02b8f0ad6a0af6592e94f9f8b3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_callbacks(name_model,montior_quan,inc_earlystopper=True,inc_checkpointer=True,inc_lr=True,init_lr=True,init_early_stopper=True,\n",
    "                 init_checkpointer=True,early_stopper=None,check_pointer=None,reduce_lr=None):\n",
    "    \n",
    "    callback=[]\n",
    "    \n",
    "    if inc_earlystopper:\n",
    "        earlystopper=None\n",
    "        # if early stopper is included then \n",
    "        if init_early_stopper:\n",
    "            # to initialize the early stopper\n",
    "            earlystopper = EarlyStopping(monitor=montior_quan,patience=15,\n",
    "                             mode='max',verbose=1)\n",
    "        else:\n",
    "            # make the early stopper given by user\n",
    "            if early_stopper!=None:\n",
    "                earlystopper=early_stopper\n",
    "            else:\n",
    "                print(\"Error Not given early stopper\")\n",
    "    \n",
    "        callback.append(earlystopper)\n",
    "        \n",
    "    if inc_lr:\n",
    "        reduce_lr=None\n",
    "        if init_lr:\n",
    "            reducelr=ReduceLROnPlateau(monitor=montior_quan,patience=5, \n",
    "                            min_lr=0.00001,mode='max',verbose=1,factor=0.5)\n",
    "        else:\n",
    "            if reduce_lr!=None:\n",
    "                reducelr=reduce_lr\n",
    "            else:\n",
    "                print(\"Error Not given reduce lr on plateau\")\n",
    "        \n",
    "        callback.append(reducelr)\n",
    "        \n",
    "    if inc_checkpointer:\n",
    "        checkpointer=None\n",
    "        if init_checkpointer:\n",
    "            checkpointer = ModelCheckpoint(name_model, monitor=montior_quan,mode='max',\n",
    "                               verbose=1, save_best_only=True)\n",
    "        else:\n",
    "            if check_pointer!=None:\n",
    "                checkpointer=check_pointer\n",
    "            else:\n",
    "                print(\"Error not given model checkpointer\")\n",
    "    \n",
    "        callback.append(checkpointer)\n",
    "        \n",
    "    return callback\n",
    "\n",
    "def fit_data(model,X_train,y_train,X_val,y_val,batches,ep,name_model,verb,monitor_quan,\n",
    "                inc_earlystopper=True,inc_checkpointer=True,inc_lr=True,init_lr=True,init_early_stopper=True,\n",
    "                 init_checkpointer=True,early_stopper=None,check_pointer=None,reduce_lr=None):\n",
    "    \n",
    "    # callbacks initialization\n",
    "    callback=create_callbacks(name_model,monitor_quan,inc_earlystopper,inc_checkpointer,inc_lr,init_lr,\n",
    "                              init_early_stopper,init_checkpointer,early_stopper,check_pointer,reduce_lr)\n",
    "    \n",
    "    \n",
    "    results = model.fit(X_train, y_train, validation_data=[X_val,y_val], \n",
    "                        batch_size=batches, epochs=ep, callbacks=callback,verbose=verb)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    \n",
    "def create_model_for_loavsz(model1):\n",
    "    # remove layter activation layer and use losvasz loss\n",
    "    input_x = model1.layers[0].input\n",
    "\n",
    "    output_layer = model1.layers[-1].input\n",
    "    model = Model(input_x, output_layer)\n",
    "    c = adam()\n",
    "\n",
    "    # lovasz_loss need input range (-∞，+∞), so cancel the last \"sigmoid\" activation  \n",
    "    # Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n",
    "    model.compile(loss=lovasz_loss, optimizer=c, metrics=[metric_for_lovasz])\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc6c39e944193242e011e843c75840ebb1362beb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b8702cb1463886a4cab6dda11385f57eef7ed31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "n_folds=5\n",
    "kfold=StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=42)\n",
    "preds_test=np.zeros((18000,101,101,1))\n",
    "preds_test_flip=np.zeros((18000,101,101,1))\n",
    "\n",
    "for fold,(train_index,val_index) in enumerate(kfold.split(X,coverage)):\n",
    "    X_train,X_val=X[train_index],X[val_index]\n",
    "    y_train,y_val=y[train_index],y[val_index]\n",
    "    \n",
    "    print(\"================================= FOLD\",fold+1,\"=============================================\")\n",
    "    \n",
    "     \n",
    "    print(X_train.shape,X_val.shape)\n",
    "    \n",
    "    # augmentation\n",
    "    print(\"Training Augmentation\")\n",
    "    X_train,y_train=augment(X_train,y_train,l=0.3,start_crop=0,end_crop=1600,\n",
    "                            start_rot=1600,end_rot=3200,start_roll=0,end_roll=4000,rolling=True)\n",
    "\n",
    "    print(\"Validation Augmentation\")\n",
    "    X_val,y_val=augment(X_val,y_val,l=0.3,start_crop=0,end_crop=800,\n",
    "                        start_rot=0,end_rot=800,start_roll=0,end_roll=3000,rolling=True)\n",
    "    \n",
    "    \n",
    "    print(\"Data shape\")\n",
    "    print(X_train.shape,X_val.shape)\n",
    "    \n",
    "    \n",
    "    model=model_call()\n",
    "    \n",
    "    save_name='models/resnet_unet_kfold_lovasz_'+str(fold+1)+'.h5'\n",
    "    \n",
    "    \n",
    "    # first it is binary cross entropy.\n",
    "    \n",
    "    batch_size=64\n",
    "    epochs=80\n",
    "    verbose=0\n",
    "    \n",
    "    monitor_quantity = 'val_competition_metric'\n",
    "    \n",
    "    result=fit_data(model,X_train,y_train,X_val,y_val,batch_size,\n",
    "                    epochs,name_model=save_name,verb=verbose,monitor_quan=monitor_quantity)\n",
    "    \n",
    "    print(\"Done with binary cross entropy\")\n",
    "    \n",
    "    # fit done and now we have to use lovasz loss\n",
    "    \n",
    "    # loading the best model\n",
    "    model1=load_model(save_name,custom_objects={'competition_metric': competition_metric})\n",
    "    \n",
    "    # creating the model for loavsz \n",
    "    model=create_model_for_loavsz(model1)\n",
    "    \n",
    "    print(\"Model ready for lovasz training\")\n",
    "    \n",
    "    batch_size=64\n",
    "    epochs=80\n",
    "    verbose=0\n",
    "    \n",
    "    monitor_quantity = 'val_metric_for_lovasz'    \n",
    "    \n",
    "    result=fit_data(model,X_train,y_train,X_val,y_val,batch_size,\n",
    "                    epochs,name_model=save_name,verb=verbose,monitor_quan=monitor_quantity)\n",
    "\n",
    "    \n",
    "    print(\"Done with training in this fold\")\n",
    "\n",
    "    # loading the final model\n",
    "    model = load_model(save_name, custom_objects={'metric_for_lovasz': metric_for_lovasz,\n",
    "                                                   'lovasz_loss': lovasz_loss})\n",
    "    \n",
    "\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(X_val,y_val))\n",
    "    \n",
    "    # making test predictions\n",
    "    preds_test=preds_test+(model.predict(X_test,verbose=1))/n_folds\n",
    "    \n",
    "    # tta\n",
    "    X_test_flip=np.array([np.fliplr(x) for x in X_test])\n",
    "    flipping_predictions=model.predict(X_test_flip,verbose=1)\n",
    "    \n",
    "    # now i have to flip these predictions\n",
    "    flipping_predictions=np.array([np.fliplr(x) for x in flipping_predictions])\n",
    "    \n",
    "    # saving the predictions \n",
    "    preds_test_flip=preds_test_flip+flipping_predictions/n_folds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ab8516fb8ab135872dd4f4b895b5d76206df1fa"
   },
   "source": [
    "## Test Data\n",
    "First we'll get the test data. This takes a while, it's 18000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds_name=\"resnet_5_fold_64_batch_loavsz\"\n",
    "# saving the test predictions\n",
    "name=\"test_predicitons/\"+preds_name\n",
    "np.save(name,preds_test)\n",
    "\n",
    "# saving the flip predictions\n",
    "name=\"test_predicitons/\"+preds_name+\"tta\"\n",
    "np.save(name,preds_test_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "844cded40edc71652bc5b26852245e37f46f6448"
   },
   "source": [
    "## Prepare Submission\n",
    "We need to prepare the submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1-mask, 0-background\n",
    "    Returns run length as string\n",
    "    '''\n",
    "    pixels = im.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73336f76166028ba39c8164083c9564a0d5afe40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold=0.0\n",
    "sub=sample.copy()\n",
    "\n",
    "for index,name in tqdm.tqdm_notebook(enumerate(sub['id'].values),total=sub['id'].shape[0]):\n",
    "    downsampled_img=preds_test[index].reshape(101,101)\n",
    "    downsampled_img=downsampled_img>threshold\n",
    "    sub.loc[index,'rle_mask']=rle_encode(downsampled_img)\n",
    "\n",
    "print(\"Number of empty predictions are\",sub.loc[sub['rle_mask']==\"\"].shape[0])\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_sub=sample.copy()\n",
    "\n",
    "preds_test_final=(preds_test+preds_test_flip)/2\n",
    "\n",
    "for index,name in tqdm.tqdm_notebook(enumerate(tta_sub['id'].values),total=tta_sub['id'].shape[0]):\n",
    "    downsampled_img=preds_test_final[index].reshape(101,101)\n",
    "    downsampled_img=downsampled_img>threshold\n",
    "    tta_sub.loc[index,'rle_mask']=rle_encode(downsampled_img)\n",
    "\n",
    "print(\"Number of empty predictions are\",tta_sub.loc[tta_sub['rle_mask']==\"\"].shape[0])\n",
    "tta_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_sub=sub.copy()\n",
    "c=0\n",
    "\n",
    "wrong_preds_ids=[]\n",
    "for i in indextest:\n",
    "    if sub.loc[i,'rle_mask']!=\"\":\n",
    "        wrong_preds_ids.append(sub['id'][i])\n",
    "        \n",
    "        # correcting the predictions\n",
    "        corrected_sub.loc[i,'rle_mask']=\"\"\n",
    "        \n",
    "        c=c+1\n",
    "\n",
    "print(c,\"predicitions are wrong\")\n",
    "print(\"After correction\")\n",
    "print(\"Number of final empty predictions are\",corrected_sub.loc[corrected_sub['rle_mask']==\"\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING THE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename=\"resnet_2_blocks_unet_64_batch_5_folds_inc_rolling_lovasz.csv\"\n",
    "\n",
    "sub.to_csv(save_filename,index=False)\n",
    "\n",
    "tta_sub.to_csv(\"tta_\"+save_filename,index=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THRESHOLD CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.05\n",
    "tta_sub=sample.copy()\n",
    "\n",
    "preds_test_final=(preds_test+preds_test_flip)/2\n",
    "\n",
    "for index,name in tqdm.tqdm_notebook(enumerate(tta_sub['id'].values),total=tta_sub['id'].shape[0]):\n",
    "    downsampled_img=downsample(preds_test_final[index].reshape(128,128))\n",
    "    downsampled_img=downsampled_img>threshold\n",
    "    tta_sub.loc[index,'rle_mask']=rle_encode(downsampled_img)\n",
    "\n",
    "print(\"Number of empty predictions are\",tta_sub.loc[tta_sub['rle_mask']==\"\"].shape[0])\n",
    "tta_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_sub.to_csv(\"tta_0.47_threshold_\"+save_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
